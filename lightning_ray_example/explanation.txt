model.py - файл с определением отдельный слоев и моделей 
+ lightning модель, которая собирает все вместе, не требует написания train loopА 
и позволяет логировать метрики прямо в tensorboard 

tokenization.py - кастомный токенизатор, который собирает в себе разные варианты токенизации 
под нашу задачу (чтобы можно было удобно подбирать гиперпараметры токенизатора вместе с остальными)

dataset.py - датасет для нашего кейса + специальный lightning датасет, который собирает 
в себе тренировочные, валидационный и тестовый датасеты

main.py - собираем все вместе + тюним гмперпараметры с помощью ray tune, который позволяет параллелить этот процесс
