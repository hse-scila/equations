{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import datasets\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"minbpe-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"pairs_dataset.csv\")\n",
    "df2 = pd.read_csv(\"generated_pairs.csv\")\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>equation</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y\\prime= \\frac{3}{x^3+x}, \\;\\;\\;\\; y(1)=0</td>\n",
       "      <td>y = 3 \\ln x -\\frac{3}{2} \\ln {(x^2+1)} + \\frac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y\\prime=3xy</td>\n",
       "      <td>y = C e^{\\frac{3}{2} x^2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\frac{dy}{dx}= xy^2 + 4x + 2y^2 + 8</td>\n",
       "      <td>y =2 \\tan{(x^2 +4x +2C)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\frac{dy}{dx}= e^{x+2y}, y(0)=1</td>\n",
       "      <td>y = -\\frac{1}{2} \\ln{(-2 e^x + 2+e^{-2})}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>y\\prime = x e^{2x+y}</td>\n",
       "      <td>y = - \\ln {(-\\frac{1}{2} x e^{2x} + \\frac{1}{4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9530</th>\n",
       "      <td>y^{\\prime\\prime\\prime}-23y^{\\prime\\prime}+166y...</td>\n",
       "      <td>C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{4x}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9531</th>\n",
       "      <td>y^{\\prime\\prime\\prime}-24y^{\\prime\\prime}+185y...</td>\n",
       "      <td>C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{5x}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532</th>\n",
       "      <td>y^{\\prime\\prime\\prime}-25y^{\\prime\\prime}+204y...</td>\n",
       "      <td>C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{6x}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>y^{\\prime\\prime\\prime}-26y^{\\prime\\prime}+223y...</td>\n",
       "      <td>C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{7x}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9534</th>\n",
       "      <td>y^{\\prime\\prime\\prime}-27y^{\\prime\\prime}+242y...</td>\n",
       "      <td>C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{8x}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10670 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               equation  \\\n",
       "0             y\\prime= \\frac{3}{x^3+x}, \\;\\;\\;\\; y(1)=0   \n",
       "1                                           y\\prime=3xy   \n",
       "2                   \\frac{dy}{dx}= xy^2 + 4x + 2y^2 + 8   \n",
       "3                      \\frac{dy}{dx}= e^{x+2y}, y(0)=1    \n",
       "4                                 y\\prime = x e^{2x+y}    \n",
       "...                                                 ...   \n",
       "9530  y^{\\prime\\prime\\prime}-23y^{\\prime\\prime}+166y...   \n",
       "9531  y^{\\prime\\prime\\prime}-24y^{\\prime\\prime}+185y...   \n",
       "9532  y^{\\prime\\prime\\prime}-25y^{\\prime\\prime}+204y...   \n",
       "9533  y^{\\prime\\prime\\prime}-26y^{\\prime\\prime}+223y...   \n",
       "9534  y^{\\prime\\prime\\prime}-27y^{\\prime\\prime}+242y...   \n",
       "\n",
       "                                                 answer  \n",
       "0     y = 3 \\ln x -\\frac{3}{2} \\ln {(x^2+1)} + \\frac...  \n",
       "1                             y = C e^{\\frac{3}{2} x^2}  \n",
       "2                              y =2 \\tan{(x^2 +4x +2C)}  \n",
       "3             y = -\\frac{1}{2} \\ln{(-2 e^x + 2+e^{-2})}  \n",
       "4     y = - \\ln {(-\\frac{1}{2} x e^{2x} + \\frac{1}{4...  \n",
       "...                                                 ...  \n",
       "9530               C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{4x}  \n",
       "9531               C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{5x}  \n",
       "9532               C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{6x}  \n",
       "9533               C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{7x}  \n",
       "9534               C_{1}e^{10x}+C_{2}e^{9x}+C_{3}e^{8x}  \n",
       "\n",
       "[10670 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # выкинули строки с короткими ответами\n",
    "# for i, row in df.iterrows():\n",
    "#     if len(row['answer']) < 5:\n",
    "#         df = df.drop([i])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add special tokens\n",
    "lines = \"<sos>\" + df[\"equation\"] +' ' + df[\"answer\"] + \"<eos>\"\n",
    "str_for_vocab_training = lines.str.cat(sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "VOCAB_SIZE = 4096\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "# doesn't allow adding special tokens; make it learn <pad>\n",
    "str_for_vocab_training += \"<pad>\" * 10\n",
    "tokenizer.train(str_for_vocab_training, vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eq_lines = \"<sos>\" + df[\"equation\"] + \"<eos>\"\n",
    "ans_lines = \"<sos>\" + df[\"answer\"] + \"<eos>\"\n",
    "eqs_tokenized = np.array(eq_lines.apply(lambda line: list(tokenizer.encode(line))))\n",
    "ans_tokenized = np.array(ans_lines.apply(lambda line: list(tokenizer.encode(line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = max(len(max(eqs_tokenized, key=len)), len(max(ans_tokenized, key=len)) ) # max seq len\n",
    "\n",
    "def pad(lines_tokenized_basic):\n",
    "    for i in range(len(lines_tokenized_basic)):\n",
    "        while len(lines_tokenized_basic[i]) < seq_length:\n",
    "            lines_tokenized_basic[i].append(tokenizer.encode(\"<pad>\")[0])\n",
    "        lines_tokenized_basic[i] = np.array(lines_tokenized_basic[i])\n",
    "    lines_tokenized_basic = np.array(lines_tokenized_basic)\n",
    "    return lines_tokenized_basic\n",
    "\n",
    "eqs_tokenized = pad(eqs_tokenized)\n",
    "ans_tokenized = pad(ans_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10670"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eqs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset_dict = {\"eqs\": eqs_tokenized, \"ans\": ans_tokenized}\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.2)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_index = tokenizer.encode(\"<pad>\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_eqs = [example[\"eqs\"] for example in batch]\n",
    "        batch_ans = [example[\"ans\"] for example in batch]\n",
    "        batch_eqs = nn.utils.rnn.pad_sequence(batch_eqs, padding_value=pad_index)\n",
    "        batch_ans = nn.utils.rnn.pad_sequence(batch_ans, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"eqs\": batch_eqs,\n",
    "            \"ans\": batch_ans,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = ['eqs', 'ans']\n",
    "\n",
    "train_data = train_test_valid_dataset['train'].with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = train_test_valid_dataset['valid'].with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = train_test_valid_dataset['test'].with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, rnn_type, input_dim, embedding_dim, hidden_dim, n_layers, dropout, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, rnn_type, output_dim, embedding_dim, hidden_dim, n_layers, dropout, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm.tqdm(data_loader)):\n",
    "        src = batch[\"eqs\"].to(device)\n",
    "        trg = batch[\"ans\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval loop\n",
    "\n",
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"eqs\"].to(device)\n",
    "            trg = batch[\"ans\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detokenize(sentence):\n",
    "    full_str = tokenizer.decode(sentence)\n",
    "    answer = full_str[5:]\n",
    "    end_token_idx = answer.find(\"<eos>\")\n",
    "    answer = answer[:end_token_idx]\n",
    "    answer = answer.replace(\"<pad>\", \"\")\n",
    "    return answer\n",
    "\n",
    "def tokenize(sentence):\n",
    "    res = tokenizer.encode('<sos>' + sentence + '<eos>')\n",
    "    while len(res) < seq_length:\n",
    "        res.append(tokenizer.encode(\"<pad>\")[0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    input_is_tokenized=False,\n",
    "    device='cuda',\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if input_is_tokenized:\n",
    "            ids = sentence\n",
    "        else:\n",
    "            ids = tokenize(sentence)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = tokenizer.encode('<sos>')\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == tokenizer.encode('<eos>')[0]:\n",
    "                break\n",
    "        tokens = detokenize(inputs)[4:]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "val_references=[detokenize(sentence.tolist()) for sentence in valid_data['ans']]\n",
    "test_references=[detokenize(sentence.tolist()) for sentence in test_data['ans']]\n",
    "\n",
    "def bleu_score(preds, refs):\n",
    "    return bleu.compute(predictions=preds, references=refs)['bleu']\n",
    "\n",
    "def accuracy(preds, refs):\n",
    "    equal_count = 0\n",
    "    for i in range(len(refs)):\n",
    "        if preds[i] == refs[i]:\n",
    "            equal_count += 1\n",
    "    return equal_count/len(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result saving\n",
    "\n",
    "result = {'rnn_type': [], 'n_layers': [], 'hidden_dim': [], 'epoch': [], 'val_bleu': [], 'val_accuracy': [], 'test_bleu': [], 'test_accuracy': []}\n",
    "\n",
    "def update_result():\n",
    "    result['rnn_type'].append(rnn_type)\n",
    "    result['n_layers'].append(n_layers)\n",
    "    result['hidden_dim'].append(hidden_dim)\n",
    "    result['epoch'].append(epoch+1)\n",
    "    result['val_bleu'].append(round(val_bleu, 3))\n",
    "    result['test_bleu'].append(round(test_bleu, 3))\n",
    "    result['val_accuracy'].append(round(val_accuracy, 3))\n",
    "    result['test_accuracy'].append(round(test_accuracy, 3))\n",
    "    \n",
    "    res_df = pd.DataFrame(result)\n",
    "    res_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,255,168 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:13<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.314 | Train PPL:  74.774\n",
      "\tValid Loss:   3.460 | Valid PPL:  31.824\n",
      "\tValid BLEU:   0.254 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:14<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.295 | Train PPL:  26.990\n",
      "\tValid Loss:   4.193 | Valid PPL:  66.188\n",
      "\tValid BLEU:   0.216 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:13<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.306 | Train PPL:  27.282\n",
      "\tValid Loss:   3.274 | Valid PPL:  26.417\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:14<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.164 | Train PPL:  23.657\n",
      "\tValid Loss:   3.242 | Valid PPL:  25.592\n",
      "\tValid BLEU:   0.418 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:14<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.135 | Train PPL:  22.979\n",
      "\tValid Loss:   3.232 | Valid PPL:  25.339\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:14<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.204 | Train PPL:  24.630\n",
      "\tValid Loss:   3.249 | Valid PPL:  25.775\n",
      "\tValid BLEU:   0.413 | Valid Accuracy:   0.000\n",
      "The model has 7,360,512 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:17<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.339 | Train PPL:  76.641\n",
      "\tValid Loss:   3.557 | Valid PPL:  35.052\n",
      "\tValid BLEU:   0.272 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:17<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.355 | Train PPL:  28.653\n",
      "\tValid Loss:   3.313 | Valid PPL:  27.457\n",
      "\tValid BLEU:   0.371 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:17<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.104 | Train PPL:  22.286\n",
      "\tValid Loss:   3.146 | Valid PPL:  23.247\n",
      "\tValid BLEU:   0.420 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:17<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.959 | Train PPL:  19.278\n",
      "\tValid Loss:   3.010 | Valid PPL:  20.292\n",
      "\tValid BLEU:   0.375 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:17<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.914 | Train PPL:  18.424\n",
      "\tValid Loss:   2.996 | Valid PPL:  20.010\n",
      "\tValid BLEU:   0.374 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:17<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.891 | Train PPL:  18.010\n",
      "\tValid Loss:   2.977 | Valid PPL:  19.637\n",
      "\tValid BLEU:   0.199 | Valid Accuracy:   0.000\n",
      "The model has 9,465,856 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:22<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.281 | Train PPL:  72.333\n",
      "\tValid Loss:   3.515 | Valid PPL:  33.631\n",
      "\tValid BLEU:   0.255 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:22<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.313 | Train PPL:  27.475\n",
      "\tValid Loss:   3.317 | Valid PPL:  27.582\n",
      "\tValid BLEU:   0.418 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:22<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.204 | Train PPL:  24.642\n",
      "\tValid Loss:   3.271 | Valid PPL:  26.348\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:21<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.165 | Train PPL:  23.683\n",
      "\tValid Loss:   3.246 | Valid PPL:  25.693\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:21<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.143 | Train PPL:  23.172\n",
      "\tValid Loss:   3.233 | Valid PPL:  25.365\n",
      "\tValid BLEU:   0.216 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:22<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.131 | Train PPL:  22.896\n",
      "\tValid Loss:   3.234 | Valid PPL:  25.377\n",
      "\tValid BLEU:   0.216 | Valid Accuracy:   0.000\n",
      "The model has 11,571,200 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:25<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.322 | Train PPL:  75.318\n",
      "\tValid Loss:   3.565 | Valid PPL:  35.342\n",
      "\tValid BLEU:   0.254 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:25<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.347 | Train PPL:  28.417\n",
      "\tValid Loss:   3.335 | Valid PPL:  28.064\n",
      "\tValid BLEU:   0.440 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:25<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.215 | Train PPL:  24.908\n",
      "\tValid Loss:   3.279 | Valid PPL:  26.550\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:25<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.170 | Train PPL:  23.813\n",
      "\tValid Loss:   3.253 | Valid PPL:  25.868\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.149 | Train PPL:  23.323\n",
      "\tValid Loss:   3.235 | Valid PPL:  25.411\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.132 | Train PPL:  22.919\n",
      "\tValid Loss:   3.229 | Valid PPL:  25.266\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "The model has 11,554,816 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.875 | Train PPL:  48.165\n",
      "\tValid Loss:   3.338 | Valid PPL:  28.162\n",
      "\tValid BLEU:   0.420 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.197 | Train PPL:  24.459\n",
      "\tValid Loss:   3.246 | Valid PPL:  25.697\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.088 | Train PPL:  21.925\n",
      "\tValid Loss:   3.029 | Valid PPL:  20.678\n",
      "\tValid BLEU:   0.355 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.895 | Train PPL:  18.084\n",
      "\tValid Loss:   2.976 | Valid PPL:  19.605\n",
      "\tValid BLEU:   0.375 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.948 | Train PPL:  19.073\n",
      "\tValid Loss:   2.996 | Valid PPL:  20.011\n",
      "\tValid BLEU:   0.390 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.862 | Train PPL:  17.491\n",
      "\tValid Loss:   2.982 | Valid PPL:  19.719\n",
      "\tValid BLEU:   0.379 | Valid Accuracy:   0.000\n",
      "The model has 19,959,808 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:29<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.823 | Train PPL:  45.735\n",
      "\tValid Loss:   3.328 | Valid PPL:  27.882\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:29<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.236 | Train PPL:  25.439\n",
      "\tValid Loss:   3.780 | Valid PPL:  43.808\n",
      "\tValid BLEU:   0.371 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:29<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.167 | Train PPL:  23.730\n",
      "\tValid Loss:   3.241 | Valid PPL:  25.556\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:29<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.137 | Train PPL:  23.042\n",
      "\tValid Loss:   3.228 | Valid PPL:  25.236\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:29<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.119 | Train PPL:  22.619\n",
      "\tValid Loss:   3.221 | Valid PPL:  25.043\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:29<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.111 | Train PPL:  22.447\n",
      "\tValid Loss:   3.221 | Valid PPL:  25.047\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "The model has 28,364,800 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.807 | Train PPL:  45.030\n",
      "\tValid Loss:   3.323 | Valid PPL:  27.738\n",
      "\tValid BLEU:   0.254 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.196 | Train PPL:  24.426\n",
      "\tValid Loss:   3.260 | Valid PPL:  26.037\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.150 | Train PPL:  23.348\n",
      "\tValid Loss:   3.241 | Valid PPL:  25.558\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.135 | Train PPL:  22.998\n",
      "\tValid Loss:   3.233 | Valid PPL:  25.344\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.121 | Train PPL:  22.667\n",
      "\tValid Loss:   3.218 | Valid PPL:  24.986\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.109 | Train PPL:  22.388\n",
      "\tValid Loss:   3.220 | Valid PPL:  25.037\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "The model has 36,769,792 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:48<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.854 | Train PPL:  47.203\n",
      "\tValid Loss:   3.338 | Valid PPL:  28.158\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:48<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.198 | Train PPL:  24.474\n",
      "\tValid Loss:   3.250 | Valid PPL:  25.787\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:48<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.147 | Train PPL:  23.274\n",
      "\tValid Loss:   3.239 | Valid PPL:  25.518\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:48<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.132 | Train PPL:  22.927\n",
      "\tValid Loss:   3.225 | Valid PPL:  25.161\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:48<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.120 | Train PPL:  22.653\n",
      "\tValid Loss:   3.221 | Valid PPL:  25.044\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:48<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.108 | Train PPL:  22.380\n",
      "\tValid Loss:   3.213 | Valid PPL:  24.857\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "The model has 33,591,296 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.649 | Train PPL:  38.432\n",
      "\tValid Loss:   3.298 | Valid PPL:  27.064\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.167 | Train PPL:  23.731\n",
      "\tValid Loss:   3.246 | Valid PPL:  25.688\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.131 | Train PPL:  22.901\n",
      "\tValid Loss:   3.234 | Valid PPL:  25.383\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.205 | Train PPL:  24.646\n",
      "\tValid Loss:   3.229 | Valid PPL:  25.265\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.092 | Train PPL:  22.030\n",
      "\tValid Loss:   3.224 | Valid PPL:  25.134\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:39<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.080 | Train PPL:  21.758\n",
      "\tValid Loss:   3.206 | Valid PPL:  24.685\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "The model has 67,178,496 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:12<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.611 | Train PPL:  37.005\n",
      "\tValid Loss:   3.292 | Valid PPL:  26.889\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:12<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.174 | Train PPL:  23.891\n",
      "\tValid Loss:   3.253 | Valid PPL:  25.861\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:12<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.139 | Train PPL:  23.071\n",
      "\tValid Loss:   3.234 | Valid PPL:  25.382\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.000\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:11<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.139 | Train PPL:  23.075\n",
      "\tValid Loss:   3.222 | Valid PPL:  25.079\n",
      "\tValid BLEU:   0.438 | Valid Accuracy:   0.000\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:12<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.107 | Train PPL:  22.356\n",
      "\tValid Loss:   3.208 | Valid PPL:  24.726\n",
      "\tValid BLEU:   0.439 | Valid Accuracy:   0.000\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:12<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.093 | Train PPL:  22.040\n",
      "\tValid Loss:   3.217 | Valid PPL:  24.944\n",
      "\tValid BLEU:   0.419 | Valid Accuracy:   0.001\n",
      "The model has 100,765,696 trainable parameters\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/67 [00:02<02:18,  2.09s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 7.78 GiB total capacity; 5.08 GiB already allocated; 180.62 MiB free; 5.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate_fn(\n\u001b[1;32m     56\u001b[0m         model,\n\u001b[1;32m     57\u001b[0m         valid_data_loader,\n\u001b[1;32m     58\u001b[0m         criterion,\n\u001b[1;32m     59\u001b[0m         device,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# trg = [(trg length - 1) * batch size]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/hugging_face/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/hugging_face/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 7.78 GiB total capacity; 5.08 GiB already allocated; 180.62 MiB free; 5.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "input_dim = VOCAB_SIZE\n",
    "output_dim = VOCAB_SIZE\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rnn_type_options = ['lstm']\n",
    "n_layers_options = [2, 4, 6, 8]\n",
    "hidden_dim_options = [256, 512, 1024]\n",
    "for rnn_type in rnn_type_options:\n",
    "    for hidden_dim in hidden_dim_options:\n",
    "        for n_layers in n_layers_options:\n",
    "            encoder = Encoder(\n",
    "                rnn_type,\n",
    "                input_dim,\n",
    "                encoder_embedding_dim,\n",
    "                hidden_dim,\n",
    "                n_layers,\n",
    "                encoder_dropout,\n",
    "            )\n",
    "\n",
    "            decoder = Decoder(\n",
    "                rnn_type,\n",
    "                output_dim,\n",
    "                decoder_embedding_dim,\n",
    "                hidden_dim,\n",
    "                n_layers,\n",
    "                decoder_dropout,\n",
    "            )\n",
    "\n",
    "            model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "            model.apply(init_weights)\n",
    "            print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "            n_epochs = 6\n",
    "            clip = 1.0\n",
    "            teacher_forcing_ratio = 0.1 # 0.5\n",
    "            best_valid_loss = float(\"inf\")\n",
    "            for epoch in range(n_epochs):\n",
    "                print(f'EPOCH {epoch+1}')\n",
    "                train_loss = train_fn(\n",
    "                    model,\n",
    "                    train_data_loader,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    clip,\n",
    "                    teacher_forcing_ratio,\n",
    "                    device,\n",
    "                )\n",
    "                valid_loss = evaluate_fn(\n",
    "                    model,\n",
    "                    valid_data_loader,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                )\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    torch.save(model.state_dict(), f\"encoder_decoder_models/model_layers-{n_layers}_epoch-{epoch+1}.pt\")\n",
    "                print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "                print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")\n",
    "\n",
    "                # compute metrics\n",
    "                val_predictions = [translate_sentence(sentence, model, True) for sentence in valid_data['eqs']]\n",
    "                test_predictions = [translate_sentence(sentence, model, True) for sentence in test_data['eqs']]\n",
    "                val_bleu = bleu_score(preds=val_predictions, refs=val_references)\n",
    "                test_bleu = bleu_score(preds=test_predictions, refs=test_references)\n",
    "                val_accuracy = accuracy(preds=val_predictions, refs=val_references)\n",
    "                test_accuracy = accuracy(preds=test_predictions, refs=test_references)\n",
    "\n",
    "                print(f\"\\tValid BLEU: {val_bleu:7.3f} | Valid Accuracy: {val_accuracy:7.3f}\")\n",
    "                update_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Hugging Face)",
   "language": "python",
   "name": "python_hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
